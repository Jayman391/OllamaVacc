# Pipeline for LLM Inference with the Vermont Advanced Computing Center

This project is a pipeline to generate LLM requests and send them to an Ollama server using resources from the Vermont Advanced Computing Center. The current use case revolves around classification, but anything within the bounds of LLM inference is fair game.

## Features

- **Flexible Inference:**  
  Leverage the power of large language models (LLMs) for a variety of inference tasks, with classification as the primary use case.
  
- **Customizable Request Generation:**  
  Easily configure how requests are generated by modifying the `generate_prompt` function in `pipeline.py` or by creating your own custom function.

- **Dynamic LLM Integration:**  
  Switch between different LLMs by changing the `model` field in the prompt in `pipeline.py` and updating the corresponding `ollama pull` command in `run_requests.sh`.

- **Scalable Infrastructure:**  
  Utilize the advanced computational resources provided by the GPU cluster within the Vermont Advanced Computing Center.

## Getting Started

### Prerequisites

- **Python 3.x:** Ensure you have Python installed on your system.
- **Bash:** For running shell scripts.
- **Ollama Server:** Installed and configured for sending and receiving LLM requests.
- **Vermont Advanced Computing Center Access:** Ensure you have access to the necessary computing resources.

### Installation

Clone the repository and install the dependencies:

```bash
git clone https://github.com/your-username/your-repository.git
cd your-repository
pip install -r requirements.txt
```

### Configuration

#### Customizing Request Generation
To tailor the request generation process to your needs, modify the generate_prompt function in pipeline.py or create your own function.

#### Changing LLM Models
To change the language model used for inference:

Update the model field in the prompt generated in pipeline.py.
Modify the ollama pull command in run_requests.sh accordingly.

### Usage

Update permissions and run the following slurm command
```bash
chmod +x run_requests.sh
sbatch requests_job.sh
```
